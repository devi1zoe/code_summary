# code_summary
Aiming at the attention redundancy problem of the self-attention mechanism in the traditional Transformer model, this paper constructs a sparse attention mechanism based on logic graph, which improves the quality of long code summary generation by guiding attention to the information on the code structure.
